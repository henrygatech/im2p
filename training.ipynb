{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EncoderCNN\n",
    "from dataLoader import build_vocab, get_loader\n",
    "import dataLoader\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = 'data/vocab.pkl'\n",
    "image_dir = 'data/images'\n",
    "caption_json = 'data/captions.json'\n",
    "data_json = 'data/val_split.json'\n",
    "batch_size = 3\n",
    "resize = 256\n",
    "crop_size = 224\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(resize),\n",
    "    transforms.RandomCrop(crop_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "data_loader = get_loader(image_dir=image_dir,\n",
    "                         caption_json=caption_json,\n",
    "                         data_json=data_json,\n",
    "                         vocabulary=vocab,\n",
    "                         transform=transform,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample a caption length, and sample indices with that length.\n",
    "(images, image_id, target, prob) = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 224, 224])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 15])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(features): <class 'torch.Tensor'>\n",
      "features.shape: torch.Size([3, 256])\n"
     ]
    }
   ],
   "source": [
    "# Specify the dimensionality of the image embedding.\n",
    "\n",
    "\n",
    "#-#-#-# Do NOT modify the code below this line. #-#-#-#\n",
    "\n",
    "# Initialize the encoder. (Optional: Add additional arguments if necessary.)\n",
    "encoder = EncoderCNN(embed_size)\n",
    "\n",
    "# Move the encoder to GPU if CUDA is available.\n",
    "encoder.to(device)\n",
    "    \n",
    "# Move last batch of images (from Step 2) to GPU if CUDA is available.   \n",
    "images = images.to(device)\n",
    "\n",
    "# Pass the images through the encoder.\n",
    "features = encoder(images)\n",
    "\n",
    "print('type(features):', type(features))\n",
    "print('features.shape:', features.shape)\n",
    "\n",
    "# Check that your encoder satisfies some requirements of the project! :D\n",
    "assert type(features)==torch.Tensor, \"Encoder output needs to be a PyTorch Tensor.\" \n",
    "assert (features.shape[0]==batch_size) & (features.shape[1]==embed_size), \"The shape of the encoder output is incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SentenceRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(features): <class 'torch.Tensor'>\n",
      "features.shape: torch.Size([3, 256])\n"
     ]
    }
   ],
   "source": [
    "# Specify the dimensionality of the image embedding.\n",
    "hiddem_size = 256\n",
    "\n",
    "#-#-#-# Do NOT modify the code below this line. #-#-#-#\n",
    "\n",
    "# Initialize the encoder. (Optional: Add additional arguments if necessary.)\n",
    "sentRnn = SentenceRNN(256,256,256)\n",
    "\n",
    "# Move the encoder to GPU if CUDA is available.\n",
    "sentRnn.to(device)\n",
    "    \n",
    "# Move last batch of images (from Step 2) to GPU if CUDA is available.   \n",
    "images = images.to(device)\n",
    "\n",
    "# Pass the images through the encoder.\n",
    "features = encoder(images)\n",
    "\n",
    "print('type(features):', type(features))\n",
    "print('features.shape:', features.shape)\n",
    "\n",
    "probs, topic, hiddens = sentRnn(features = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import WordRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import WordRNN, SentenceRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 256])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 18])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n",
      "torch.Size([3, 1, 256])\n",
      "topics torch.Size([3, 1, 256])\n",
      "embedding torch.Size([3, 19, 256])\n",
      "inner inputs torch.Size([3, 20, 256])\n",
      "type(outputs): <class 'torch.Tensor'>\n",
      "outputs.shape: torch.Size([3, 20, 4667])\n"
     ]
    }
   ],
   "source": [
    "# Specify the number of features in the hidden state of the RNN decoder.\n",
    "hidden_size = 512\n",
    "\n",
    "#-#-#-# Do NOT modify the code below this line. #-#-#-#\n",
    "\n",
    "# Store the size of the vocabulary.\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Initialize the decoder.\n",
    "wordRnn = WordRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move the decoder to GPU if CUDA is available.\n",
    "wordRnn.to(device)\n",
    " \n",
    "# Move last batch of captions (from Step 1) to GPU if CUDA is available \n",
    "target_0 = target[:,0,:].to(device)\n",
    "topic = topic.to(device)\n",
    "print(target_0.shape)\n",
    "print(topic.shape)\n",
    "\n",
    "# Pass the encoder output and captions through the decoder.\n",
    "outputs = wordRnn(topic, target_0)\n",
    "\n",
    "print('type(outputs):', type(outputs))\n",
    "print('outputs.shape:', outputs.shape)\n",
    "\n",
    "# Check that your decoder satisfies some requirements of the project! \n",
    "assert type(outputs)==torch.Tensor, \"Decoder output needs to be a PyTorch Tensor.\"\n",
    "assert (outputs.shape[0]==batch_size) & (outputs.shape[1]==target_0.shape[1]) & (outputs.shape[2]==vocab_size), \"The shape of the decoder output is incorrect.\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 18, 4667])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 18])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_max = 6\n",
    "n_max = 50\n",
    "states = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 2])\n",
      "tensor([[[0.5179, 0.4821]],\n",
      "\n",
      "        [[0.4756, 0.5244]],\n",
      "\n",
      "        [[0.4941, 0.5059]]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([3, 2])\n",
      "tensor([[1, 0],\n",
      "        [0, 1],\n",
      "        [0, 1]], device='cuda:0', dtype=torch.uint8)\n",
      "(3, 50)\n",
      "tensor([[1],\n",
      "        [0],\n",
      "        [0]], device='cuda:0', dtype=torch.uint8)\n",
      "tensor([[2589., 2437., 1423.,  540.,  618., 4498.,  577.,  140.,  701., 2128.,\n",
      "         3688.,   96., 3841., 4473., 1564., 2486., 1455., 4657., 4593., 2525.,\n",
      "         4358., 3038., 3270., 2296., 2296., 3365., 1014., 2012.,  275., 2894.,\n",
      "         4281., 2814.,  956., 4091., 1694., 3925., 3571., 1751., 4643., 2490.,\n",
      "         2490., 4296., 1930., 1500., 3848., 1691., 2791., 1118., 4214., 1244.],\n",
      "        [2589., 2437., 1423.,  540.,  618., 4498.,  577.,  140.,  701., 2128.,\n",
      "         3688.,   96., 3841., 4473., 1564., 2486., 1455., 4657., 4593., 2525.,\n",
      "         4358., 3038., 3270., 2296., 2296., 3365., 1014., 2012.,  275., 2894.,\n",
      "         4281., 2814.,  956., 4091., 1694., 3925., 3571., 1751., 4643., 2490.,\n",
      "         2490., 4296., 1930., 1500., 3848., 1691., 2791., 1118., 4214., 1244.],\n",
      "        [2589., 2437., 1423.,  540.,  618., 4498.,  577.,  140.,  701., 2128.,\n",
      "         3688.,   96., 3841., 4473., 1564., 2486., 1455., 4657., 4593., 2525.,\n",
      "         4358., 3038., 3270., 2296., 2296., 3365., 1014., 2012.,  275., 2894.,\n",
      "         4281., 2814.,  956., 4091., 1694., 3925., 3571., 1751., 4643., 2490.,\n",
      "         2490., 4296., 1930., 1500., 3848., 1691., 2791., 1118., 4214., 1244.]],\n",
      "       device='cuda:0')\n",
      "[[2589. 2437. 1423.  540.  618. 4498.  577.  140.  701. 2128. 3688.   96.\n",
      "  3841. 4473. 1564. 2486. 1455. 4657. 4593. 2525. 4358. 3038. 3270. 2296.\n",
      "  2296. 3365. 1014. 2012.  275. 2894. 4281. 2814.  956. 4091. 1694. 3925.\n",
      "  3571. 1751. 4643. 2490. 2490. 4296. 1930. 1500. 3848. 1691. 2791. 1118.\n",
      "  4214. 1244.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(s_max):\n",
    "    p, topic, states = sentRnn.sample(features, states)\n",
    "    samples_ids = wordRnn.sample(topic, max_len=n_max)\n",
    "    print(p)\n",
    "    p = (p > 0.5).squeeze(1)\n",
    "    print(p.shape)\n",
    "    print(p)\n",
    "    print(samples_ids.shape)\n",
    "    print(p[:,0].view(3,1))\n",
    "    print(torch.Tensor(samples_ids).to(device))\n",
    "    samples_ids = samples_ids * p[:,0].cpu().data.numpy().reshape(3,1)\n",
    "    print(samples_ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
